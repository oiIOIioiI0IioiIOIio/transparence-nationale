#!/usr/bin/env python3
"""
Script de rÃ©cupÃ©ration des dÃ©clarations HATVP complÃ¨tes des Ã©lus franÃ§ais.

TÃ©lÃ©charge le fichier XML unique HATVP (declarations.xml) contenant TOUTES
les dÃ©clarations publiÃ©es, puis extrait rÃ©cursivement TOUTES les informations
de chaque dÃ©claration pour chaque Ã©lu prÃ©sent dans elus.json.

Sections extraites (DSP â€” patrimoine) :
  - biens immobiliers, comptes bancaires, instruments financiers
  - participations financiÃ¨res, vÃ©hicules, biens mobiliers de valeur
  - dettes/emprunts, revenus
Sections extraites (DI â€” intÃ©rÃªts) :
  - activitÃ©s professionnelles, activitÃ©s antÃ©rieures, mandats Ã©lectifs
  - participations dans des organes, fonctions bÃ©nÃ©voles
  - autres liens d'intÃ©rÃªts

Sources :
  Index CSV     : https://www.hatvp.fr/livraison/opendata/liste.csv
  XML (toutes)  : https://www.hatvp.fr/livraison/opendata/declarations.xml
  Doc officielle: https://www.hatvp.fr/open-data/

Utilisation :
  python generate-elus.py --dry-run
  python generate-elus.py --limit 50
  python generate-elus.py --test-elu "YaÃ«l Braun-Pivet"
  python generate-elus.py --force
  python generate-elus.py --dump-xml-sample   # affiche un XML brut pour debug
"""

import argparse
import csv
import io
import json
import os
import re
import time
import unicodedata
import urllib.error
import urllib.parse
import urllib.request
import xml.etree.ElementTree as ET
from datetime import datetime

# â”€â”€ Chemins â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SCRIPT_DIR   = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(SCRIPT_DIR)
OUTPUT_JSON  = os.path.join(PROJECT_ROOT, "public", "data", "elus.json")
CACHE_DIR    = os.path.join(PROJECT_ROOT, "public", "data", "hatvp_cache")
INDEX_CACHE  = os.path.join(CACHE_DIR, "liste.csv")
XML_CACHE    = os.path.join(CACHE_DIR, "declarations.xml")

# â”€â”€ URLs HATVP open data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
HATVP_INDEX_URL = "https://www.hatvp.fr/livraison/opendata/liste.csv"

# Le XML unique contenant TOUTES les dÃ©clarations
HATVP_DECLARATIONS_XML_URL = "https://www.hatvp.fr/livraison/opendata/declarations.xml"

# Fallback : XMLs individuels par nom_fichier (colonne du CSV)
HATVP_DOSSIER_BASE = "https://www.hatvp.fr/livraison/dossiers/"

# â”€â”€ Colonnes rÃ©elles du CSV HATVP (notice officielle) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# civilite;prenom;nom;classement;type_mandat;qualite;type_document;departement;
# date_publication;nom_fichier;url_dossier;id_origine;url_photo
#
# type_document : DI, DSP, DSPFIN, DIMAJ, etc.
# nom_fichier   : nom du PDF (souvent aussi base du XML)
# url_dossier   : slug URL vers la page de la dÃ©claration

# Types de dÃ©claration Ã  extraire
DSP_TYPES = {"DSP", "DSPM", "DSPFIN", "DSPMAJ"}
DI_TYPES  = {"DI", "DIM", "DIMAJ"}
ALL_DOC_TYPES = DSP_TYPES | DI_TYPES

# â”€â”€ Headers HTTP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
HEADERS = {
    "User-Agent": "TransparenceNationale/1.0 (open source; github.com/transparence-nationale)",
    "Accept": "text/csv, application/xml, text/xml, */*",
}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Parsing args
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def parse_args():
    p = argparse.ArgumentParser(
        description="Extrait le patrimoine complet (DSP + DI) depuis les XMLs HATVP."
    )
    p.add_argument("--dry-run",          action="store_true", help="Ne pas Ã©crire de fichiers")
    p.add_argument("--force",            action="store_true", help="Re-tÃ©lÃ©charger mÃªme si en cache")
    p.add_argument("--limit",            type=int,   default=None, help="Limiter le nombre d'Ã©lus")
    p.add_argument("--delay",            type=float, default=0.3,  help="DÃ©lai entre requÃªtes (dÃ©faut 0.3 s)")
    p.add_argument("--test-elu",         type=str,   default=None, help="Tester un Ã©lu prÃ©cis")
    p.add_argument("--refresh-index",    action="store_true",
                   help="Forcer le re-tÃ©lÃ©chargement du CSV index HATVP")
    p.add_argument("--refresh-xml",      action="store_true",
                   help="Forcer le re-tÃ©lÃ©chargement du XML complet HATVP")
    p.add_argument("--dump-xml-sample",  action="store_true",
                   help="Afficher un extrait du XML brut (debug)")
    p.add_argument("--dump-csv-columns", action="store_true",
                   help="Afficher les colonnes du CSV index (debug)")
    return p.parse_args()


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# RÃ©seau
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def http_get(url: str, timeout: int = 120) -> bytes | None:
    """TÃ©lÃ©charger une URL. Timeout Ã©levÃ© pour le gros XML (~200 Mo)."""
    req = urllib.request.Request(url, headers=HEADERS)
    try:
        with urllib.request.urlopen(req, timeout=timeout) as resp:
            if resp.status == 200:
                return resp.read()
    except urllib.error.HTTPError as exc:
        if exc.code not in (404, 403, 410):
            print(f"  âš  HTTP {exc.code} â†’ {url}")
    except Exception as exc:
        print(f"  âš  RÃ©seau : {exc}")
    return None


def download_file(url: str, cache_path: str, force: bool = False,
                  max_age_h: float = 24, delay: float = 0.3) -> bytes | None:
    """TÃ©lÃ©charger un fichier avec cache local."""
    if not force and os.path.exists(cache_path):
        age_h = (time.time() - os.path.getmtime(cache_path)) / 3600
        if age_h < max_age_h:
            print(f"  âœ“ En cache ({age_h:.1f} h) : {os.path.basename(cache_path)}")
            with open(cache_path, "rb") as f:
                return f.read()
        else:
            print(f"  â†» Cache trop ancien ({age_h:.1f} h), re-tÃ©lÃ©chargementâ€¦")

    time.sleep(delay)
    print(f"  ğŸ”„ TÃ©lÃ©chargement : {url}")
    data = http_get(url)
    if data:
        os.makedirs(os.path.dirname(cache_path), exist_ok=True)
        with open(cache_path, "wb") as f:
            f.write(data)
        print(f"  âœ“ TÃ©lÃ©chargÃ© ({len(data):,} octets) â†’ {cache_path}")
    return data


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Helpers XML gÃ©nÃ©riques
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def xml_text(element: ET.Element | None, path: str, default: str = "") -> str:
    """Extraire le texte d'un nÅ“ud XML en toute sÃ©curitÃ©."""
    if element is None:
        return default
    node = element.find(path)
    if node is not None and node.text:
        t = node.text.strip()
        if t and t not in ("[DonnÃ©es non publiÃ©es]", "null"):
            return t
    return default


def parse_montant(s: str) -> float | None:
    """Convertir une chaÃ®ne montant en float."""
    if not s:
        return None
    s = s.replace("\xa0", "").replace(" ", "").replace(",", ".").strip()
    s = re.sub(r"[^\d.\-]", "", s)
    try:
        return float(s)
    except ValueError:
        return None


def normalize_name(s: str) -> str:
    """Normaliser un nom : minuscules, sans accents, sans tirets."""
    s = unicodedata.normalize("NFD", s)
    s = "".join(c for c in s if unicodedata.category(c) != "Mn")
    s = s.lower()
    s = re.sub(r"[-\s]+", " ", s).strip()
    return s


def element_to_dict(el: ET.Element) -> dict:
    """
    Convertir rÃ©cursivement un Ã©lÃ©ment XML en dict.
    Si un Ã©lÃ©ment a des enfants, on rÃ©curse. Sinon, on prend le texte.
    GÃ¨re les listes (<items>) et les sous-objets (<nature><id>X</id></nature>).
    """
    result = {}
    children = list(el)
    if not children:
        # Feuille : retourner le texte
        t = (el.text or "").strip()
        if t and t != "[DonnÃ©es non publiÃ©es]":
            return t
        return ""

    for child in children:
        tag = child.tag
        value = element_to_dict(child)

        if tag == "items":
            # Accumuler les <items> dans une liste
            result.setdefault("_items", [])
            if isinstance(value, dict) and "_items" in value:
                # items imbriquÃ©s : <items><items>...</items></items>
                result["_items"].extend(value["_items"])
            elif value:  # Ignorer les vides
                result["_items"].append(value)
        elif tag in result:
            # Doublon (rare) : convertir en liste
            existing = result[tag]
            if isinstance(existing, list):
                existing.append(value)
            else:
                result[tag] = [existing, value]
        else:
            result[tag] = value

    return result


def flatten_section_items(section_dict: dict) -> list[dict]:
    """Extraire la liste d'items d'une section parsÃ©e rÃ©cursivement."""
    if not isinstance(section_dict, dict):
        return []
    # VÃ©rifier neant
    neant = section_dict.get("neant", "")
    if isinstance(neant, str) and neant.lower() == "true":
        return []
    items = section_dict.get("_items", [])
    # Filtrer les items qui sont des dicts non vides
    result = []
    for item in items:
        if isinstance(item, dict):
            # Aplatir les sous-objets (nature/id â†’ nature_id, nature/label â†’ nature_label)
            flat = flatten_item(item)
            if flat and any(v for v in flat.values() if v):
                result.append(flat)
    return result


def flatten_item(d: dict, prefix: str = "") -> dict:
    """
    Aplatir un dict imbriquÃ©.
    {nature: {id: "X", label: "Y"}} â†’ {nature_id: "X", nature_label: "Y"}
    """
    result = {}
    for k, v in d.items():
        if k == "_items":
            continue  # Ignorer les sous-listes imbriquÃ©es
        key = f"{prefix}{k}" if not prefix else f"{prefix}_{k}"
        if isinstance(v, dict):
            # Sous-objet (ex: nature/id, modeDetention/label)
            if "id" in v or "label" in v:
                if v.get("id"):
                    result[f"{key}_id"] = v["id"]
                if v.get("label"):
                    result[f"{key}_label"] = v["label"]
                # Garder aussi la valeur combinÃ©e
                result[key] = v.get("label") or v.get("id") or ""
            else:
                # RÃ©curser
                sub = flatten_item(v, key)
                result.update(sub)
        elif isinstance(v, list):
            # Multiple valeurs (rare)
            result[key] = "; ".join(str(x) for x in v if x)
        else:
            result[key] = v if v else ""
    return result


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Chargement de l'index CSV HATVP
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def load_hatvp_index(force_refresh: bool = False, delay: float = 0.3) -> list[dict]:
    """TÃ©lÃ©charger et parser le CSV index HATVP."""
    os.makedirs(CACHE_DIR, exist_ok=True)
    raw = download_file(HATVP_INDEX_URL, INDEX_CACHE, force=force_refresh, delay=delay)
    if not raw:
        raise RuntimeError(f"Impossible de tÃ©lÃ©charger l'index HATVP : {HATVP_INDEX_URL}")

    try:
        text = raw.decode("utf-8-sig")
    except UnicodeDecodeError:
        text = raw.decode("latin-1")

    reader = csv.DictReader(io.StringIO(text), delimiter=";")
    rows = list(reader)
    if rows:
        print(f"  âœ“ {len(rows):,} entrÃ©es â€” colonnes : {list(rows[0].keys())}")
    else:
        print("  âš  CSV vide")
    return rows


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Chargement du XML complet HATVP (declarations.xml)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def load_declarations_xml(force_refresh: bool = False, delay: float = 0.3) -> ET.Element | None:
    """
    TÃ©lÃ©charger et parser le XML unique contenant TOUTES les dÃ©clarations HATVP.
    Retourne l'Ã©lÃ©ment racine.
    """
    os.makedirs(CACHE_DIR, exist_ok=True)
    raw = download_file(
        HATVP_DECLARATIONS_XML_URL, XML_CACHE,
        force=force_refresh, max_age_h=48, delay=delay
    )
    if not raw:
        print("  âš  Impossible de tÃ©lÃ©charger le XML complet")
        return None

    print(f"  ğŸ“– Parsing du XML ({len(raw):,} octets)â€¦")
    try:
        root = ET.fromstring(raw)
    except ET.ParseError as exc:
        print(f"  âŒ XML invalide : {exc}")
        return None

    # Compter les dÃ©clarations
    declarations = root.findall(".//declaration")
    if not declarations:
        # Essayer d'autres structures possibles
        declarations = list(root)
    print(f"  âœ“ {len(declarations)} dÃ©claration(s) dans le XML")
    return root


def build_xml_index(root: ET.Element) -> dict[str, list[ET.Element]]:
    """
    Construire un index {nom_normalisÃ© -> [Ã©lÃ©ments dÃ©claration]} depuis le XML.
    Permet une recherche rapide par nom d'Ã©lu.
    """
    index = {}

    # Le XML peut avoir plusieurs structures. Essayons :
    # <declarations><declaration>...</declaration></declarations>
    # ou directement les enfants du root sont des dÃ©clarations
    declarations = root.findall(".//declaration")
    if not declarations:
        declarations = list(root)

    for decl in declarations:
        # Extraire nom/prÃ©nom du dÃ©clarant
        nom    = xml_text(decl, ".//general/declarant/nom") or xml_text(decl, ".//declarant/nom") or xml_text(decl, ".//nom") or ""
        prenom = xml_text(decl, ".//general/declarant/prenom") or xml_text(decl, ".//declarant/prenom") or xml_text(decl, ".//prenom") or ""

        if not nom:
            continue

        key = normalize_name(f"{prenom} {nom}")
        index.setdefault(key, []).append(decl)

    print(f"  âœ“ Index XML : {len(index)} personnes distinctes")
    return index


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Index CSV : correspondance Ã©lu â†” dÃ©clarations
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def find_csv_rows_for_elu(csv_index: list[dict], prenom: str, nom: str) -> list[dict]:
    """Retrouver les entrÃ©es CSV pour un Ã©lu (par nom normalisÃ©)."""
    norm_prenom = normalize_name(prenom)
    norm_nom    = normalize_name(nom)
    matched = []
    for row in csv_index:
        r_nom    = normalize_name(row.get("nom", ""))
        r_prenom = normalize_name(row.get("prenom", ""))
        if r_nom == norm_nom and r_prenom == norm_prenom:
            matched.append(row)
    # Tri par date de publication (plus rÃ©cent en premier)
    def sort_key(row):
        d = row.get("date_publication", "")
        try:
            return datetime.strptime(d.strip(), "%Y-%m-%d")
        except (ValueError, AttributeError):
            return datetime.min
    matched.sort(key=sort_key, reverse=True)
    return matched


def get_individual_xml_url(csv_row: dict) -> str | None:
    """
    Construire l'URL d'un XML individuel depuis une ligne CSV.
    La colonne 'nom_fichier' contient le nom du PDF, mais le XML
    est souvent disponible au mÃªme chemin avec extension .xml.
    La colonne 'url_dossier' contient le slug vers la fiche.
    """
    nom_fichier = (csv_row.get("nom_fichier") or "").strip()
    if nom_fichier:
        # Remplacer .pdf par .xml
        base = nom_fichier.rsplit(".", 1)[0] if "." in nom_fichier else nom_fichier
        return HATVP_DOSSIER_BASE + base + ".xml"
    return None


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Extraction rÃ©cursive complÃ¨te d'une dÃ©claration XML
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Sections connues du XML HATVP et leur catÃ©gorie
KNOWN_SECTIONS = {
    # DSP â€” Patrimoine
    "instrumentsFinanciersDto":     "instruments_financiers",
    "participationFinanciereDto":   "participations_financieres",
    "biensImmobiliersDto":          "biens_immobiliers",
    "bienImmobilierDto":            "biens_immobiliers",
    "comptesBancairesDto":          "comptes_bancaires",
    "compteBancaireDto":            "comptes_bancaires",
    "liquiditesDto":                "comptes_bancaires",
    "vehiculesDto":                 "vehicules",
    "vehiculeDto":                  "vehicules",
    "autresBiensDto":               "biens_mobiliers_valeur",
    "biensMobiliersDto":            "biens_mobiliers_valeur",
    "biensValeurDto":               "biens_mobiliers_valeur",
    "dettesDto":                    "dettes",
    "detteDto":                     "dettes",
    "empruntsDto":                  "dettes",
    "revenusDto":                   "revenus",
    "revenuDto":                    "revenus",
    "revenusActiviteDto":           "revenus",
    # DI â€” IntÃ©rÃªts
    "activitesProfessionnellesDto": "activites_professionnelles",
    "activiteProfessionnelleDto":   "activites_professionnelles",
    "fonctionsActuellesDto":        "activites_professionnelles",
    "activitesAnterieuresDto":      "activites_anterieures",
    "activiteAnterieureDto":        "activites_anterieures",
    "fonctionsAnterieuresDto":      "activites_anterieures",
    "mandatsElectifsDto":           "mandats_electifs",
    "mandatElectifDto":             "mandats_electifs",
    "mandatsDto":                   "mandats_electifs",
    "participationsOrganeDto":      "participations_organes",
    "participationOrganeDto":       "participations_organes",
    "organesDirigeantsDto":         "participations_organes",
    "fonctionsBenevolesDto":        "fonctions_benevoles",
    "soutiensAssociationsDto":      "fonctions_benevoles",
    "soutienAssociationDto":        "fonctions_benevoles",
    "activitesBenevolesDto":        "fonctions_benevoles",
    "autresLiensInteretsDto":       "autres_liens_interets",
    "autreLienInteretDto":          "autres_liens_interets",
    "liensInteretsDto":             "autres_liens_interets",
    "autresActivitesDto":           "autres_activites",
    "autreActiviteDto":             "autres_activites",
    # Sections additionnelles courantes
    "fonctionsGouvernementalesDto":  "fonctions_gouvernementales",
    "consultatifEtAutresDto":        "fonctions_consultatives",
    "participationExploitantDto":    "participations_exploitant",
}

ALL_OUTPUT_SECTIONS = sorted(set(KNOWN_SECTIONS.values()))

SECTION_LABELS = {
    "instruments_financiers":       "ğŸ“ˆ Instruments financiers",
    "participations_financieres":   "ğŸ¢ Participations dans des sociÃ©tÃ©s",
    "biens_immobiliers":            "ğŸ  Biens immobiliers",
    "comptes_bancaires":            "ğŸ¦ Comptes bancaires & Ã©pargne",
    "vehicules":                    "ğŸš— VÃ©hicules",
    "biens_mobiliers_valeur":       "ğŸ’ Biens mobiliers de valeur",
    "dettes":                       "ğŸ“‰ Dettes & emprunts",
    "revenus":                      "ğŸ’¶ Revenus",
    "activites_professionnelles":   "ğŸ’¼ ActivitÃ©s professionnelles",
    "activites_anterieures":        "ğŸ“‹ ActivitÃ©s antÃ©rieures",
    "mandats_electifs":             "ğŸ—³ï¸  Mandats Ã©lectifs",
    "participations_organes":       "ğŸ›ï¸  Participations Ã  des organes",
    "fonctions_benevoles":          "ğŸ¤ Fonctions bÃ©nÃ©voles",
    "autres_liens_interets":        "âš ï¸  Autres liens d'intÃ©rÃªts",
    "autres_activites":             "ğŸ“ Autres activitÃ©s",
    "fonctions_gouvernementales":   "ğŸ›ï¸  Fonctions gouvernementales",
    "fonctions_consultatives":      "ğŸ“‹ Fonctions consultatives",
    "participations_exploitant":    "ğŸ­ Participations exploitant",
}


def extract_declaration_data(decl_element: ET.Element) -> dict:
    """
    Extraire TOUTES les donnÃ©es d'un Ã©lÃ©ment <declaration> XML
    en parcourant rÃ©cursivement toutes les sections.
    """
    result = {
        # MÃ©tadonnÃ©es
        "type_declaration":       xml_text(decl_element, ".//general/typeDeclaration/id") or xml_text(decl_element, "typeDeclaration/id") or "",
        "type_declaration_label": xml_text(decl_element, ".//general/typeDeclaration/label") or xml_text(decl_element, "typeDeclaration/label") or "",
        "date_depot":             xml_text(decl_element, "dateDepot") or xml_text(decl_element, ".//dateDepot") or "",
        "date_publication":       xml_text(decl_element, "datePublication") or "",
        "uuid":                   xml_text(decl_element, "uuid") or xml_text(decl_element, ".//uuid") or "",
        "declarant_nom":          xml_text(decl_element, ".//general/declarant/nom") or xml_text(decl_element, ".//declarant/nom") or "",
        "declarant_prenom":       xml_text(decl_element, ".//general/declarant/prenom") or xml_text(decl_element, ".//declarant/prenom") or "",
        "qualite":                xml_text(decl_element, ".//general/qualiteDeclarant") or xml_text(decl_element, ".//qualite") or "",
        "organe":                 xml_text(decl_element, ".//general/organe/labelOrgane") or xml_text(decl_element, ".//organe") or "",
        "mandat":                 xml_text(decl_element, ".//general/qualiteMandat/labelTypeMandat") or "",
    }

    # Initialiser toutes les sections connues
    for section_name in ALL_OUTPUT_SECTIONS:
        result[section_name] = []

    # Parcourir TOUS les enfants (et descendants) de la dÃ©claration
    # pour trouver les sections connues
    for section_tag, output_key in KNOWN_SECTIONS.items():
        for section_el in decl_element.iter(section_tag):
            parsed = element_to_dict(section_el)
            items = flatten_section_items(parsed)
            result[output_key].extend(items)

    # FALLBACK : parcourir aussi rÃ©cursivement pour trouver des sections
    # qu'on n'a pas dans notre mapping mais qui contiennent des items
    seen_tags = set(KNOWN_SECTIONS.keys()) | {"general", "uuid", "dateDepot",
                "datePublication", "declaration", "declarations"}
    for child in decl_element:
        tag = child.tag
        if tag in seen_tags:
            continue
        # VÃ©rifier si cette section contient des items
        if child.find("items") is not None or child.find(".//items") is not None:
            parsed = element_to_dict(child)
            items = flatten_section_items(parsed)
            if items:
                # Stocker dans "autres_activites" par dÃ©faut
                safe_key = re.sub(r"Dto$", "", tag)
                safe_key = re.sub(r"([A-Z])", r"_\1", safe_key).lower().strip("_")
                # Utiliser la section existante la plus proche ou crÃ©er
                if safe_key not in result:
                    result[safe_key] = []
                    # L'ajouter aussi aux sections connues pour l'affichage
                    if safe_key not in ALL_OUTPUT_SECTIONS:
                        ALL_OUTPUT_SECTIONS.append(safe_key)
                result[safe_key].extend(items)

    return result


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Orchestration par Ã©lu
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def fetch_data_for_elu(
    elu: dict,
    xml_index: dict[str, list[ET.Element]],
    csv_index: list[dict],
    force: bool,
    dry_run: bool,
    delay: float,
) -> dict | None:
    """
    Extraire TOUTES les donnÃ©es HATVP d'un Ã©lu.
    Cherche d'abord dans le XML global, sinon tente les XMLs individuels.
    """
    prenom = elu.get("prenom", "").strip()
    nom    = elu.get("nom",    "").strip()
    if not prenom or not nom:
        return None

    result = {
        "prenom":                prenom,
        "nom":                   nom,
        "scraped_at":            datetime.utcnow().isoformat() + "Z",
        "declarations_trouvees": 0,
        "declarations":          [],
    }
    for section_name in ALL_OUTPUT_SECTIONS:
        result[section_name] = []

    # â”€â”€ StratÃ©gie 1 : chercher dans le XML global â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    key = normalize_name(f"{prenom} {nom}")
    xml_decls = xml_index.get(key, [])

    if xml_decls:
        result["declarations_trouvees"] = len(xml_decls)
        print(f"    âœ“ {len(xml_decls)} dÃ©claration(s) dans le XML global")

        for decl_el in xml_decls:
            if dry_run:
                result["declarations"].append({"source": "xml_global", "dry_run": True})
                continue

            parsed = extract_declaration_data(decl_el)

            # Fusionner les sections
            for section_name in list(set(ALL_OUTPUT_SECTIONS) | set(parsed.keys())):
                if section_name in (
                    "type_declaration", "type_declaration_label", "date_depot",
                    "date_publication", "uuid", "declarant_nom", "declarant_prenom",
                    "qualite", "organe", "mandat",
                ):
                    continue
                items = parsed.get(section_name, [])
                if isinstance(items, list):
                    if section_name not in result:
                        result[section_name] = []
                    result[section_name].extend(items)

            result["declarations"].append({
                "source":     "xml_global",
                "type":       parsed.get("type_declaration", ""),
                "label":      parsed.get("type_declaration_label", ""),
                "date_depot": parsed.get("date_depot", ""),
                "uuid":       parsed.get("uuid", ""),
                "qualite":    parsed.get("qualite", ""),
                "organe":     parsed.get("organe", ""),
            })

        return result

    # â”€â”€ StratÃ©gie 2 : XMLs individuels via le CSV â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    csv_rows = find_csv_rows_for_elu(csv_index, prenom, nom)
    if not csv_rows:
        print(f"    âœ— Aucune dÃ©claration HATVP trouvÃ©e pour {prenom} {nom}")
        return None

    result["declarations_trouvees"] = len(csv_rows)
    print(f"    âœ“ {len(csv_rows)} entrÃ©e(s) CSV â€” fallback XMLs individuels")

    # Prendre les plus rÃ©centes : 1 DSP + 1 DI
    fetched_types = set()
    for csv_row in csv_rows:
        doc_type = (csv_row.get("type_document") or "").strip().upper()
        if doc_type not in ALL_DOC_TYPES:
            continue
        # Ã‰viter les doublons de mÃªme catÃ©gorie
        category = "DSP" if doc_type in DSP_TYPES else "DI"
        if category in fetched_types:
            continue

        xml_url = get_individual_xml_url(csv_row)
        if not xml_url:
            continue

        print(f"    ğŸ”„ {doc_type} : {xml_url}")

        if dry_run:
            result["declarations"].append({"source": "xml_individuel", "type": doc_type, "url": xml_url, "dry_run": True})
            fetched_types.add(category)
            continue

        filename   = xml_url.split("/")[-1]
        cache_path = os.path.join(CACHE_DIR, "xmls", filename)
        xml_bytes  = download_file(xml_url, cache_path, force=force, max_age_h=168, delay=delay)

        if not xml_bytes:
            print(f"    âœ— Impossible de tÃ©lÃ©charger {xml_url}")
            continue

        try:
            decl_root = ET.fromstring(xml_bytes)
        except ET.ParseError as exc:
            print(f"    âš  XML invalide ({exc})")
            continue

        parsed = extract_declaration_data(decl_root)

        for section_name in list(set(ALL_OUTPUT_SECTIONS) | set(parsed.keys())):
            if section_name in (
                "type_declaration", "type_declaration_label", "date_depot",
                "date_publication", "uuid", "declarant_nom", "declarant_prenom",
                "qualite", "organe", "mandat",
            ):
                continue
            items = parsed.get(section_name, [])
            if isinstance(items, list):
                if section_name not in result:
                    result[section_name] = []
                result[section_name].extend(items)

        result["declarations"].append({
            "source":     "xml_individuel",
            "type":       parsed.get("type_declaration", ""),
            "label":      parsed.get("type_declaration_label", ""),
            "date_depot": parsed.get("date_depot", ""),
            "uuid":       parsed.get("uuid", ""),
            "url":        xml_url,
            "qualite":    parsed.get("qualite", ""),
            "organe":     parsed.get("organe", ""),
        })
        fetched_types.add(category)

    return result


def build_resume_hatvp(data: dict) -> dict:
    """Construire un rÃ©sumÃ© compact pour elus.json."""

    def count_and_total(items: list[dict]) -> tuple[int, float]:
        n = len(items)
        total = 0.0
        for i in items:
            for k in ("valeur_euro", "solde_euro", "montant_euro",
                       "valeur", "solde", "montant", "valeurParts",
                       "capitalRestantDu", "remuneration_euro", "indemnite_euro"):
                v = i.get(k)
                if v is not None:
                    if isinstance(v, str):
                        v = parse_montant(v)
                    if isinstance(v, (int, float)):
                        total += v
                        break
        return n, total

    resume = {
        "nb_declarations_hatvp": data.get("declarations_trouvees", 0),
        "hatvp_scraped_at":      data.get("scraped_at", ""),
    }

    patrimoine_brut = 0.0
    total_dettes    = 0.0
    total_revenus   = 0.0

    for section_name in ALL_OUTPUT_SECTIONS:
        items = data.get(section_name, [])
        if not items:
            continue
        n, total = count_and_total(items)
        resume[f"nb_{section_name}"] = n
        if total:
            resume[f"valeur_{section_name}_euro"] = total

        # Calculer patrimoine net
        if section_name == "dettes":
            total_dettes += total
        elif section_name == "revenus":
            total_revenus += total
        elif section_name not in (
            "activites_professionnelles", "activites_anterieures",
            "mandats_electifs", "participations_organes",
            "fonctions_benevoles", "autres_liens_interets",
            "autres_activites", "fonctions_gouvernementales",
            "fonctions_consultatives",
        ):
            patrimoine_brut += total

    if patrimoine_brut or total_dettes:
        resume["total_actif_brut_euro"] = patrimoine_brut
        resume["total_dettes_euro"]     = total_dettes
        resume["patrimoine_net_euro"]   = patrimoine_brut - total_dettes
    if total_revenus:
        resume["total_revenus_euro"] = total_revenus

    return resume


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# I/O elus.json
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def load_elus() -> list[dict]:
    if not os.path.exists(OUTPUT_JSON):
        print(f"âš  {OUTPUT_JSON} introuvable")
        return []
    with open(OUTPUT_JSON, "r", encoding="utf-8") as f:
        return json.load(f)


def save_elus(elus: list[dict]) -> None:
    with open(OUTPUT_JSON, "w", encoding="utf-8") as f:
        json.dump(elus, f, ensure_ascii=False, indent=2)
    print(f"âœ“ {OUTPUT_JSON} mis Ã  jour ({len(elus)} Ã©lus)")


def find_elu_by_name(elus: list[dict], query: str) -> dict | None:
    q = normalize_name(query)
    for e in elus:
        full = normalize_name(f"{e.get('prenom', '')} {e.get('nom', '')}")
        if q in full or full in q:
            return e
    return None


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Affichage
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def print_section(data: dict, section: str, nb: int = 5) -> None:
    items = data.get(section, [])
    if not items:
        return
    label = SECTION_LABELS.get(section, f"ğŸ“„ {section}")
    print(f"\n  {label} ({len(items)}) :")
    for item in items[:nb]:
        parts = []
        for k, v in item.items():
            if v and k not in ("commentaire", "_items"):
                if isinstance(v, float):
                    parts.append(f"{k}={v:,.0f} â‚¬")
                elif isinstance(v, str) and len(v) < 60:
                    parts.append(f"{k}={v}")
        print(f"    â€¢ {' | '.join(parts[:5])}")
    if len(items) > nb:
        print(f"    â€¦ et {len(items) - nb} autres")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Main
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def main():
    args = parse_args()

    print("=" * 65)
    print("ğŸ’° SCRAPER HATVP â€” PATRIMOINE COMPLET (DSP + DI)")
    print("   XML global  : declarations.xml")
    print("   Index CSV   : liste.csv")
    if args.dry_run:
        print("   âš  MODE DRY-RUN â€” aucun fichier ne sera Ã©crit")
    print("=" * 65)

    os.makedirs(CACHE_DIR, exist_ok=True)
    os.makedirs(os.path.join(CACHE_DIR, "xmls"), exist_ok=True)

    # â”€â”€ Charger le CSV index â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\nğŸ“¥ Chargement de l'index CSV HATVPâ€¦")
    csv_index = load_hatvp_index(force_refresh=args.refresh_index, delay=args.delay)

    if args.dump_csv_columns and csv_index:
        print(f"\n  ğŸ“‹ Colonnes CSV : {list(csv_index[0].keys())}")
        print(f"  ğŸ“‹ Exemple ligne 1 :")
        for k, v in csv_index[0].items():
            print(f"    {k:25s} = {v}")
        print(f"  ğŸ“‹ Exemple ligne 2 :")
        for k, v in csv_index[1].items():
            print(f"    {k:25s} = {v}")
        return

    # â”€â”€ Charger le XML global â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\nğŸ“¥ Chargement du XML global HATVPâ€¦")
    xml_root = load_declarations_xml(force_refresh=args.refresh_xml, delay=args.delay)

    if args.dump_xml_sample and xml_root is not None:
        print(f"\n  ğŸ“‹ Structure XML racine : <{xml_root.tag}> ({len(list(xml_root))} enfants)")
        for i, child in enumerate(xml_root):
            if i >= 3:
                break
            print(f"\n  â”€â”€ DÃ©claration {i+1} : <{child.tag}>")
            snippet = ET.tostring(child, encoding="unicode")[:3000]
            print(snippet)
        return

    # Construire l'index par nom
    xml_index: dict[str, list[ET.Element]] = {}
    if xml_root is not None:
        print("\nğŸ”¨ Construction de l'index par nomâ€¦")
        xml_index = build_xml_index(xml_root)

    if not xml_index and not csv_index:
        print("âŒ Aucune donnÃ©e HATVP disponible (ni XML ni CSV)")
        return

    # â”€â”€ Mode test â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if args.test_elu:
        print(f"\nğŸ§ª Mode test â€” Ã©lu : {args.test_elu}")
        elus = load_elus()
        elu  = find_elu_by_name(elus, args.test_elu)
        if not elu:
            parts = args.test_elu.strip().split()
            elu = {"id": "test", "prenom": parts[0], "nom": " ".join(parts[1:])}
        print(f"  Profil : {elu.get('prenom')} {elu.get('nom')}")

        result = fetch_data_for_elu(
            elu, xml_index, csv_index,
            force=True, dry_run=args.dry_run, delay=args.delay
        )

        if result:
            print(f"\n{'=' * 65}")
            print("âœ… RÃ‰SULTAT COMPLET")
            print(f"{'=' * 65}")
            print(f"  DÃ©clarations trouvÃ©es : {result['declarations_trouvees']}")

            total_items = sum(
                len(result.get(s, []))
                for s in ALL_OUTPUT_SECTIONS
                if isinstance(result.get(s), list)
            )
            print(f"  Total Ã©lÃ©ments extraits : {total_items}")

            for section in ALL_OUTPUT_SECTIONS:
                print_section(result, section)

            # Sections dynamiques (non prÃ©dÃ©finies)
            for k, v in result.items():
                if isinstance(v, list) and v and k not in ALL_OUTPUT_SECTIONS and k != "declarations":
                    print_section(result, k)

            print(f"\n  ğŸ“Š RÃ©sumÃ© patrimoine :")
            resume = build_resume_hatvp(result)
            print(json.dumps(resume, ensure_ascii=False, indent=4))
        else:
            print("  âœ— Aucune donnÃ©e rÃ©cupÃ©rÃ©e")
        return

    # â”€â”€ Mode batch â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    elus = load_elus()
    if not elus:
        print("âš  elus.json vide ou introuvable. Utilisez --test-elu pour tester.")
        return

    if args.limit:
        elus = elus[: args.limit]

    total     = len(elus)
    done      = 0
    not_found = 0
    with_data = 0
    updated: dict[str, dict] = {}

    for i, elu in enumerate(elus, 1):
        prenom = elu.get("prenom", "")
        nom    = elu.get("nom",    "")
        elu_id = elu.get("id",     f"elu-{i}")
        print(f"\n[{i}/{total}] {prenom} {nom}")

        result = fetch_data_for_elu(
            elu, xml_index, csv_index,
            force=args.force, dry_run=args.dry_run, delay=args.delay
        )

        if result is None:
            not_found += 1
        else:
            done += 1
            resume = build_resume_hatvp(result)
            updated[elu_id] = resume

            total_items = sum(
                len(result.get(s, []))
                for s in ALL_OUTPUT_SECTIONS
                if isinstance(result.get(s), list)
            )

            if total_items:
                with_data += 1
                summary_parts = [
                    f"{len(result[s])} {s.replace('_', ' ')}"
                    for s in ALL_OUTPUT_SECTIONS
                    if isinstance(result.get(s), list) and result.get(s)
                ]
                print(f"  âœ“ {total_items} Ã©lÃ©ments : {', '.join(summary_parts)}")
            else:
                print(f"  â—‹ DÃ©clarations trouvÃ©es mais aucun Ã©lÃ©ment dÃ©clarÃ©")

            # Sauvegarder le dÃ©tail complet
            if not args.dry_run:
                detail_path = os.path.join(CACHE_DIR, f"{elu_id}.json")
                with open(detail_path, "w", encoding="utf-8") as f:
                    json.dump(result, f, ensure_ascii=False, indent=2)

    # ï¿½ï¿½ï¿½â”€ Mettre Ã  jour elus.json â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if not args.dry_run and updated:
        all_elus = load_elus()
        for e in all_elus:
            if e.get("id") in updated:
                e["hatvp"] = updated[e["id"]]
        save_elus(all_elus)

    print("\n" + "=" * 65)
    print("ğŸ“Š RAPPORT FINAL")
    print("=" * 65)
    print(f"  Total traitÃ©s              : {total}")
    print(f"  âœ“ TrouvÃ©s dans HATVP       : {done}")
    print(f"  âœ“ Avec donnÃ©es financiÃ¨res : {with_data}")
    print(f"  âœ— Non trouvÃ©s              : {not_found}")
    print(f"  DÃ©tails dans               : {CACHE_DIR}/")
    print("=" * 65)


if __name__ == "__main__":
    main()
